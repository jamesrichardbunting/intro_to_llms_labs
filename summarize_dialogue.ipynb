{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Generative AI Use Case: Summarize Dialogue\n","\n","In this lab, I will perform \"dialogue summarization\" with generative AI, expploring how prompt engineering affects the model, using zero shot, one shot, and few shot inferences. "]},{"cell_type":"markdown","metadata":{},"source":["# Table of Contents"]},{"cell_type":"markdown","metadata":{},"source":["- [ 1 - Set up Kernel and Required Dependencies](#1)\n","- [ 2 - Summarize Dialogue without Prompt Engineering](#2)\n","- [ 3 - Summarize Dialogue with an Instruction Prompt](#3)\n","  - [ 3.1 - Zero Shot Inference with an Instruction Prompt](#3.1)\n","  - [ 3.2 - Zero Shot Inference with the Prompt Template from FLAN-T5](#3.2)\n","- [ 4 - Summarize Dialogue with One Shot and Few Shot Inference](#4)\n","  - [ 4.1 - One Shot Inference](#4.1)\n","  - [ 4.2 - Few Shot Inference](#4.2)\n","- [ 5 - Generative Configuration Parameters for Inference](#5)"]},{"cell_type":"markdown","metadata":{},"source":["<a name='1'></a>\n","## 1 - Install Required Dependencies\n","\n","Install the packages required to use PyTorch and Hugging Face transformers and datasets.\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pip in /usr/local/python/3.12.1/lib/python3.12/site-packages (24.2)\n","Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (73.0.1)\n","Requirement already satisfied: wheel in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.44.0)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: datasets in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.21.0)\n","Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (2.1.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (4.66.5)\n","Requirement already satisfied: xxhash in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (3.10.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (0.24.6)\n","Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: pip in /usr/local/python/3.12.1/lib/python3.12/site-packages (24.2)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: torch in /home/codespace/.local/lib/python3.12/site-packages (2.4.0+cpu)\n","Requirement already satisfied: torchdata in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.8.0)\n","Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (4.9.0)\n","Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.12/site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch) (2023.10.0)\n","Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch) (73.0.1)\n","Requirement already satisfied: urllib3>=1.25 in /home/codespace/.local/lib/python3.12/site-packages (from torchdata) (2.2.2)\n","Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from torchdata) (2.32.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->torchdata) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->torchdata) (3.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->torchdata) (2024.7.4)\n","Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: transformers in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.44.2)\n","Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers) (0.24.6)\n","Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.1.0)\n","Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers) (2024.7.24)\n","Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install --upgrade pip setuptools wheel\n","\n","%pip install -U datasets\n","\n","%pip install --upgrade pip\n","%pip install --disable-pip-version-check \\\n","    torch \\\n","    torchdata\n","\n","%pip install \\\n","    transformers\n","    "]},{"cell_type":"markdown","metadata":{},"source":["Load the datasets, Large Language Model (LLM), tokenizer, and configurator. "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from datasets import load_dataset\n","from transformers import AutoModelForSeq2SeqLM\n","from transformers import AutoTokenizer\n","from transformers import GenerationConfig"]},{"cell_type":"markdown","metadata":{},"source":["<a name='2'></a>\n","## 2 - Summarize Dialogue without Prompt Engineering\n","\n","In this use case, I'll be generating a summary of dialogue with the pre-trained LLM \"FLAN-T5\" from Hugging Face. The list of available models in the Hugging Face `transformers` package can be found [here](https://huggingface.co/docs/transformers/index). \n","\n","I'll upload some simple dialogue from the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. This dataset contains 10,000+ dialogues with the corresponding manually labeled summaries and topics. "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading readme: 100%|██████████| 4.65k/4.65k [00:00<00:00, 13.1kB/s]\n"]}],"source":["huggingface_dataset_name = \"knkarthick/dialogsum\"\n","\n","dataset = load_dataset(huggingface_dataset_name)"]},{"cell_type":"markdown","metadata":{},"source":["Print a couple of dialogues with their baseline summaries."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","Example  1\n","---------------------------------------------------------------------------------------------------\n","INPUT DIALOGUE:\n","#Person1#: I don't know how to adjust my life. Would you give me a piece of advice?\n","#Person2#: You look a bit pale, don't you?\n","#Person1#: Yes, I can't sleep well every night.\n","#Person2#: You should get plenty of sleep.\n","#Person1#: I drink a lot of wine.\n","#Person2#: If I were you, I wouldn't drink too much.\n","#Person1#: I often feel so tired.\n","#Person2#: You better do some exercise every morning.\n","#Person1#: I sometimes find the shadow of death in front of me.\n","#Person2#: Why do you worry about your future? You're very young, and you'll make great contribution to the world. I hope you take my advice.\n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person2# hopes #Person1# will become healthy and positive.\n","---------------------------------------------------------------------------------------------------\n","\n","---------------------------------------------------------------------------------------------------\n","Example  2\n","---------------------------------------------------------------------------------------------------\n","INPUT DIALOGUE:\n","#Person1#: Oh dear, my weight has gone up again.\n","#Person2#: I am not surprised, you eat too much.\n","#Person1#: And I suppose sitting at the desk all day at the office doesn't help.\n","#Person2#: No, I wouldn't think so.\n","#Person1#: I do wish I could lose weight.\n","#Person2#: Well, why don't you go on a diet?\n","#Person1#: I've tried diets before but they've never worked.\n","#Person2#: Perhaps you should exercise more. Why don't you go to an exercise class.\n","#Person1#: Yes, maybe I should.\n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# wants to lose weight. #Person2# suggests #Person1# take an exercise class to exercise more.\n","---------------------------------------------------------------------------------------------------\n","\n"]}],"source":["example_indices = [44, 204]\n","\n","dash_line = '-'.join('' for x in range(100))\n","\n","for i, index in enumerate(example_indices):\n","    print(dash_line)\n","    print('Example ', i + 1)\n","    print(dash_line)\n","    print('INPUT DIALOGUE:')\n","    print(dataset['test'][index]['dialogue'])\n","    print(dash_line)\n","    print('BASELINE HUMAN SUMMARY:')\n","    print(dataset['test'][index]['summary'])\n","    print(dash_line)\n","    print()"]},{"cell_type":"markdown","metadata":{},"source":["Load the [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5), creating an instance of the `AutoModelForSeq2SeqLM` class with the `.from_pretrained()` method. "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["model_name='google/flan-t5-base'\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"]},{"cell_type":"markdown","metadata":{},"source":["To perform encoding and decoding, we need to work with text in a tokenized form. **Tokenization** is the process of splitting texts into smaller units that can be processed by the LLM models. \n","\n","I'll start by downloading the tokenizer for the FLAN-T5 model using `AutoTokenizer.from_pretrained()` method. The parameter `use_fast` switches on fast tokenize, which offers performance and efficiency benefits. "]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"]},{"cell_type":"markdown","metadata":{},"source":["Test the tokenizer's encoding/decoding with a sinple sentence: "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ENCODED SENTENCE:\n","tensor([2372,  625, 3575,   58,    1])\n","\n","DECODED SENTENCE:\n","Any old iron?\n"]}],"source":["sentence = \"Any old iron?\"\n","\n","sentence_encoded = tokenizer(sentence, return_tensors='pt')\n","\n","sentence_decoded = tokenizer.decode(\n","        sentence_encoded[\"input_ids\"][0], \n","        skip_special_tokens=True\n","    )\n","\n","print('ENCODED SENTENCE:')\n","print(sentence_encoded[\"input_ids\"][0])\n","print('\\nDECODED SENTENCE:')\n","print(sentence_decoded)"]},{"cell_type":"markdown","metadata":{},"source":["Now I can explore how well the base LLM summarizes dialogue without any prompt engineering. **Prompt engineering** is an act of a human changing the **prompt** (input) to improve the response for a given task."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","Example  1\n","---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","#Person1#: I don't know how to adjust my life. Would you give me a piece of advice?\n","#Person2#: You look a bit pale, don't you?\n","#Person1#: Yes, I can't sleep well every night.\n","#Person2#: You should get plenty of sleep.\n","#Person1#: I drink a lot of wine.\n","#Person2#: If I were you, I wouldn't drink too much.\n","#Person1#: I often feel so tired.\n","#Person2#: You better do some exercise every morning.\n","#Person1#: I sometimes find the shadow of death in front of me.\n","#Person2#: Why do you worry about your future? You're very young, and you'll make great contribution to the world. I hope you take my advice.\n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person2# hopes #Person1# will become healthy and positive.\n","---------------------------------------------------------------------------------------------------\n","MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n","Person1: I'm worried about my future.\n","\n","---------------------------------------------------------------------------------------------------\n","Example  2\n","---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","#Person1#: Oh dear, my weight has gone up again.\n","#Person2#: I am not surprised, you eat too much.\n","#Person1#: And I suppose sitting at the desk all day at the office doesn't help.\n","#Person2#: No, I wouldn't think so.\n","#Person1#: I do wish I could lose weight.\n","#Person2#: Well, why don't you go on a diet?\n","#Person1#: I've tried diets before but they've never worked.\n","#Person2#: Perhaps you should exercise more. Why don't you go to an exercise class.\n","#Person1#: Yes, maybe I should.\n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# wants to lose weight. #Person2# suggests #Person1# take an exercise class to exercise more.\n","---------------------------------------------------------------------------------------------------\n","MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n","#Person1#: I'm not surprised, you eat too much. #Person2#: I'm not surprised, you eat too much. #Person1#: I wish I could lose weight. #\n","\n"]}],"source":["for i, index in enumerate(example_indices):\n","    dialogue = dataset['test'][index]['dialogue']\n","    summary = dataset['test'][index]['summary']\n","    \n","    inputs = tokenizer(dialogue, return_tensors='pt')\n","    output = tokenizer.decode(\n","        model.generate(\n","            inputs[\"input_ids\"], \n","            max_new_tokens=50,\n","        )[0], \n","        skip_special_tokens=True\n","    )\n","    \n","    print(dash_line)\n","    print('Example ', i + 1)\n","    print(dash_line)\n","    print(f'INPUT PROMPT:\\n{dialogue}')\n","    print(dash_line)\n","    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n","    print(dash_line)\n","    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')"]},{"cell_type":"markdown","metadata":{},"source":["The model's responses make some sense, but they don't seem to be reflect the goal summarizsation. Let's test if prompt engineering can help by providing examples for it to learn from. "]},{"cell_type":"markdown","metadata":{},"source":["<a name='3'></a>\n","## 3 - Summarize Dialogue with an Instruction Prompt\n","\n","In [this blog](https://www.amazon.science/blog/emnlp-prompt-engineering-is-the-new-feature-engineering) from Amazon Science, prompt engineering is discussed in more detail. "]},{"cell_type":"markdown","metadata":{},"source":["<a name='3.1'></a>\n","### 3.1 - Zero Shot Inference with an Instruction Prompt\n","\n","In order to instruct the model to perform a task - summarise a dialogue - we can convert the dialogue into an instruction prompt. This is often called **zero shot inference**.  In [this blog from AWS](https://aws.amazon.com/blogs/machine-learning/zero-shot-prompting-for-the-flan-t5-foundation-model-in-amazon-sagemaker-jumpstart/) zero shot learning and why it is an important concept to LLM models, are explained.\n","\n","Wrap the dialogue in a descriptive instruction and see how the generated text will change:"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","Example  1\n","---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","\n","Summarize the following conversation:\n","\n","#Person1#: I don't know how to adjust my life. Would you give me a piece of advice?\n","#Person2#: You look a bit pale, don't you?\n","#Person1#: Yes, I can't sleep well every night.\n","#Person2#: You should get plenty of sleep.\n","#Person1#: I drink a lot of wine.\n","#Person2#: If I were you, I wouldn't drink too much.\n","#Person1#: I often feel so tired.\n","#Person2#: You better do some exercise every morning.\n","#Person1#: I sometimes find the shadow of death in front of me.\n","#Person2#: Why do you worry about your future? You're very young, and you'll make great contribution to the world. I hope you take my advice.\n","\n","    \n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person2# hopes #Person1# will become healthy and positive.\n","---------------------------------------------------------------------------------------------------\n","MODEL GENERATION - ZERO SHOT:\n","#Person1#: I'm worried about my future.\n","\n","---------------------------------------------------------------------------------------------------\n","Example  2\n","---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","\n","Summarize the following conversation:\n","\n","#Person1#: Oh dear, my weight has gone up again.\n","#Person2#: I am not surprised, you eat too much.\n","#Person1#: And I suppose sitting at the desk all day at the office doesn't help.\n","#Person2#: No, I wouldn't think so.\n","#Person1#: I do wish I could lose weight.\n","#Person2#: Well, why don't you go on a diet?\n","#Person1#: I've tried diets before but they've never worked.\n","#Person2#: Perhaps you should exercise more. Why don't you go to an exercise class.\n","#Person1#: Yes, maybe I should.\n","\n","    \n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# wants to lose weight. #Person2# suggests #Person1# take an exercise class to exercise more.\n","---------------------------------------------------------------------------------------------------\n","MODEL GENERATION - ZERO SHOT:\n","#Person1#: I'm not sure what to do.\n","\n"]}],"source":["for i, index in enumerate(example_indices):\n","    dialogue = dataset['test'][index]['dialogue']\n","    summary = dataset['test'][index]['summary']\n","\n","    prompt = f\"\"\"\n","Summarize the following conversation:\n","\n","{dialogue}\n","\n","    \"\"\"\n","\n","    # Input constructed prompt instead of the dialogue.\n","    inputs = tokenizer(prompt, return_tensors='pt')\n","    output = tokenizer.decode(\n","        model.generate(\n","            inputs[\"input_ids\"], \n","            max_new_tokens=50,\n","        )[0], \n","        skip_special_tokens=True\n","    )\n","    \n","    print(dash_line)\n","    print('Example ', i + 1)\n","    print(dash_line)\n","    print(f'INPUT PROMPT:\\n{prompt}')\n","    print(dash_line)\n","    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n","    print(dash_line)    \n","    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"]},{"cell_type":"markdown","metadata":{},"source":["This is different, but doesn't really seem to be picking up on much more detail or nuance. \n","\n","**Exercise:**\n","\n","- Experiment with the `prompt` text and see how the inferences change. Do they change if you end the prompt with just an empty string vs. `Summary: `?\n","- Try to rephrase the beginning of the `prompt` text from `Summarize the following conversation.` to something different - and see how it will influence the generated output."]},{"cell_type":"markdown","metadata":{},"source":["<a name='3.2'></a>\n","### 3.2 - Zero Shot Inference with the Prompt Template from FLAN-T5\n","\n","Now I'll try a different prompt. FLAN-T5 has many prompt templates that are published for certain tasks [here](https://github.com/google-research/FLAN/tree/main/flan/v2). In the following code, I'll use a [pre-built FLAN-T5 prompt](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py):"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","Example  1\n","---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","\n","Dialogue:\n","\n","#Person1#: I don't know how to adjust my life. Would you give me a piece of advice?\n","#Person2#: You look a bit pale, don't you?\n","#Person1#: Yes, I can't sleep well every night.\n","#Person2#: You should get plenty of sleep.\n","#Person1#: I drink a lot of wine.\n","#Person2#: If I were you, I wouldn't drink too much.\n","#Person1#: I often feel so tired.\n","#Person2#: You better do some exercise every morning.\n","#Person1#: I sometimes find the shadow of death in front of me.\n","#Person2#: Why do you worry about your future? You're very young, and you'll make great contribution to the world. I hope you take my advice.\n","\n","What was going on?\n","\n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person2# hopes #Person1# will become healthy and positive.\n","\n","---------------------------------------------------------------------------------------------------\n","MODEL GENERATION - ZERO SHOT:\n","Person1 is worried about his future.\n","\n","---------------------------------------------------------------------------------------------------\n","Example  2\n","---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","\n","Dialogue:\n","\n","#Person1#: Oh dear, my weight has gone up again.\n","#Person2#: I am not surprised, you eat too much.\n","#Person1#: And I suppose sitting at the desk all day at the office doesn't help.\n","#Person2#: No, I wouldn't think so.\n","#Person1#: I do wish I could lose weight.\n","#Person2#: Well, why don't you go on a diet?\n","#Person1#: I've tried diets before but they've never worked.\n","#Person2#: Perhaps you should exercise more. Why don't you go to an exercise class.\n","#Person1#: Yes, maybe I should.\n","\n","What was going on?\n","\n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# wants to lose weight. #Person2# suggests #Person1# take an exercise class to exercise more.\n","\n","---------------------------------------------------------------------------------------------------\n","MODEL GENERATION - ZERO SHOT:\n","Person1 is overweight and has a lot of food.\n","\n"]}],"source":["for i, index in enumerate(example_indices):\n","    dialogue = dataset['test'][index]['dialogue']\n","    summary = dataset['test'][index]['summary']\n","        \n","    prompt = f\"\"\"\n","Dialogue:\n","\n","{dialogue}\n","\n","What was going on?\n","\"\"\"\n","\n","    inputs = tokenizer(prompt, return_tensors='pt')\n","    output = tokenizer.decode(\n","        model.generate(\n","            inputs[\"input_ids\"], \n","            max_new_tokens=50,\n","        )[0], \n","        skip_special_tokens=True\n","    )\n","\n","    print(dash_line)\n","    print('Example ', i + 1)\n","    print(dash_line)\n","    print(f'INPUT PROMPT:\\n{prompt}')\n","    print(dash_line)\n","    print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n","    print(dash_line)\n","    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"]},{"cell_type":"markdown","metadata":{},"source":["The prompt from FLAN-T5 really helped to improve the output and make it closer to the kind of summary I'd expect, although it still struggled to pick up on the nuance of the conversation. I'll try to solve this with \"few shot\" inferencing."]},{"cell_type":"markdown","metadata":{},"source":["<a name='4'></a>\n","## 4 - Summarise Dialogue with One Shot and Few Shot Inference\n","\n","**One shot and few shot inference** are the practices of providing an LLM with either one or more full examples of prompt-response pairs that match your task within the same prompt where you outline the task you want performed. This is called \"in-context learning\" and helps a model understand the specifics of your task.  You can read more about it in [this blog from HuggingFace](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api)."]},{"cell_type":"markdown","metadata":{},"source":["<a name='4.1'></a>\n","### 4.1 - One Shot Inference\n","\n","I'll build a function that takes a list of `example_indices_full`, generates a prompt with full examples, then at the end appends the prompt which I want the model to complete (`example_index_to_summarize`).  I'll use the same FLAN-T5 prompt template from section [3.2](#3.2). "]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["def make_prompt(example_indices_full, example_index_to_summarize):\n","    prompt = ''\n","    for index in example_indices_full:\n","        dialogue = dataset['test'][index]['dialogue']\n","        summary = dataset['test'][index]['summary']\n","        \n","        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n","        prompt += f\"\"\"\n","Dialogue:\n","\n","{dialogue}\n","\n","What was going on?\n","{summary}\n","\n","\n","\"\"\"\n","    \n","    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n","    \n","    prompt += f\"\"\"\n","Dialogue:\n","\n","{dialogue}\n","\n","What was going on?\n","\"\"\"\n","        \n","    return prompt"]},{"cell_type":"markdown","metadata":{},"source":["Construct the prompt to perform one shot inference:"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Dialogue:\n","\n","#Person1#: I don't know how to adjust my life. Would you give me a piece of advice?\n","#Person2#: You look a bit pale, don't you?\n","#Person1#: Yes, I can't sleep well every night.\n","#Person2#: You should get plenty of sleep.\n","#Person1#: I drink a lot of wine.\n","#Person2#: If I were you, I wouldn't drink too much.\n","#Person1#: I often feel so tired.\n","#Person2#: You better do some exercise every morning.\n","#Person1#: I sometimes find the shadow of death in front of me.\n","#Person2#: Why do you worry about your future? You're very young, and you'll make great contribution to the world. I hope you take my advice.\n","\n","What was going on?\n","#Person2# hopes #Person1# will become healthy and positive.\n","\n","\n","\n","Dialogue:\n","\n","#Person1#: Oh dear, my weight has gone up again.\n","#Person2#: I am not surprised, you eat too much.\n","#Person1#: And I suppose sitting at the desk all day at the office doesn't help.\n","#Person2#: No, I wouldn't think so.\n","#Person1#: I do wish I could lose weight.\n","#Person2#: Well, why don't you go on a diet?\n","#Person1#: I've tried diets before but they've never worked.\n","#Person2#: Perhaps you should exercise more. Why don't you go to an exercise class.\n","#Person1#: Yes, maybe I should.\n","\n","What was going on?\n","\n"]}],"source":["example_indices_full = [44]\n","example_index_to_summarize = 204\n","\n","one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n","\n","print(one_shot_prompt)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# wants to lose weight. #Person2# suggests #Person1# take an exercise class to exercise more.\n","\n","---------------------------------------------------------------------------------------------------\n","MODEL GENERATION - ONE SHOT:\n","Person1 is overweight and has a lot of food.\n"]}],"source":["summary = dataset['test'][example_index_to_summarize]['summary']\n","\n","inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n","output = tokenizer.decode(\n","    model.generate(\n","        inputs[\"input_ids\"],\n","        max_new_tokens=50,\n","    )[0], \n","    skip_special_tokens=True\n",")\n","\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n","print(dash_line)\n","print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"]},{"cell_type":"markdown","metadata":{},"source":["<a name='4.2'></a>\n","### 4.2 - Few Shot Inference\n","\n","Let's explore few shot inference by adding two more full dialogue-summary pairs to the prompt."]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Dialogue:\n","\n","#Person1#: I don't know how to adjust my life. Would you give me a piece of advice?\n","#Person2#: You look a bit pale, don't you?\n","#Person1#: Yes, I can't sleep well every night.\n","#Person2#: You should get plenty of sleep.\n","#Person1#: I drink a lot of wine.\n","#Person2#: If I were you, I wouldn't drink too much.\n","#Person1#: I often feel so tired.\n","#Person2#: You better do some exercise every morning.\n","#Person1#: I sometimes find the shadow of death in front of me.\n","#Person2#: Why do you worry about your future? You're very young, and you'll make great contribution to the world. I hope you take my advice.\n","\n","What was going on?\n","#Person2# hopes #Person1# will become healthy and positive.\n","\n","\n","\n","Dialogue:\n","\n","#Person1#: OK, that's a cut! Let's start from the beginning, everyone.\n","#Person2#: What was the problem that time?\n","#Person1#: The feeling was all wrong, Mike. She is telling you that she doesn't want to see you any more, but I want to get more anger from you. You're acting hurt and sad, but that's not how your character would act in this situation.\n","#Person2#: But Jason and Laura have been together for three years. Don't you think his reaction would be one of both anger and sadness?\n","#Person1#: At this point, no. I think he would react the way most guys would, and then later on, we would see his real feelings.\n","#Person2#: I'm not so sure about that.\n","#Person1#: Let's try it my way, and you can see how you feel when you're saying your lines. After that, if it still doesn't feel right, we can try something else.\n","\n","What was going on?\n","#Person1# and Mike are discussing what kind of emotion should be expressed by Mike in this play. They have different understandings.\n","\n","\n","\n","Dialogue:\n","\n","#Person1#: Oh dear, my weight has gone up again.\n","#Person2#: I am not surprised, you eat too much.\n","#Person1#: And I suppose sitting at the desk all day at the office doesn't help.\n","#Person2#: No, I wouldn't think so.\n","#Person1#: I do wish I could lose weight.\n","#Person2#: Well, why don't you go on a diet?\n","#Person1#: I've tried diets before but they've never worked.\n","#Person2#: Perhaps you should exercise more. Why don't you go to an exercise class.\n","#Person1#: Yes, maybe I should.\n","\n","What was going on?\n","\n"]}],"source":["example_indices_full = [44, 99]\n","example_index_to_summarize = 204\n","\n","few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n","\n","print(few_shot_prompt)"]},{"cell_type":"markdown","metadata":{},"source":["Now pass this prompt to perform a few shot inference:"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# wants to lose weight. #Person2# suggests #Person1# take an exercise class to exercise more.\n","\n","---------------------------------------------------------------------------------------------------\n","MODEL GENERATION - FEW SHOT:\n","Person1 is overweight and has a lot of food.\n"]}],"source":["summary = dataset['test'][example_index_to_summarize]['summary']\n","\n","inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n","output = tokenizer.decode(\n","    model.generate(\n","        inputs[\"input_ids\"],\n","        max_new_tokens=50,\n","    )[0], \n","    skip_special_tokens=True\n",")\n","\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n","print(dash_line)\n","print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"]},{"cell_type":"markdown","metadata":{},"source":["In this case, few shot did not provide much of an improvement over one shot inference (typically, anything above 5- or 6-shot will stop yielding improvements). However, feeding in at least one full example (one shot) seemed to provides the model with more information and qualitatively improve the summary output.\n","\n","Running the previous cell also raised some warnings regarding the input-context lengthh: generally, it should be ensured that the model's input-context length isn't exceeded (in this case, 512 tokens) as anything above the context length will be ignored.\n","\n","**Exercise:**\n","\n","Experiment with the few shot inferencing.\n","- Choose different dialogues - change the indices in the `example_indices_full` list and `example_index_to_summarize` value.\n","- Change the number of shots. Be sure to stay within the model's 512 context length, however.\n","\n","How well does few shot inferencing work with other examples?"]},{"cell_type":"markdown","metadata":{},"source":["<a name='5'></a>\n","## 5 - Generative Configuration Parameters for Inference"]},{"cell_type":"markdown","metadata":{},"source":["You can change the configuration parameters of the `generate()` method to see a different output from the LLM. So far the only parameter that you have been setting was `max_new_tokens=50`, which defines the maximum number of tokens to generate. A full list of available parameters can be found in the [Hugging Face Generation documentation](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig). \n","\n","A convenient way of organizing the configuration parameters is to use `GenerationConfig` class. "]},{"cell_type":"markdown","metadata":{},"source":["**Exercise:**\n","\n","Change the configuration parameters to investigate their influence on the output. \n","\n","Putting the parameter `do_sample = True`, you activate various decoding strategies which influence the next token from the probability distribution over the entire vocabulary. You can then adjust the outputs changing `temperature` and other parameters (such as `top_k` and `top_p`). \n","\n","Uncomment the lines in the cell below and rerun the code. Try to analyze the results. You can read some comments below."]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","MODEL GENERATION - FEW SHOT:\n","#Asked: Whose weight is going up again?\n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# wants to lose weight. #Person2# suggests #Person1# take an exercise class to exercise more.\n","\n"]}],"source":["# generation_config = GenerationConfig(max_new_tokens=50)\n","# generation_config = GenerationConfig(max_new_tokens=10)\n","# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n","# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n","generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n","\n","inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n","output = tokenizer.decode(\n","    model.generate(\n","        inputs[\"input_ids\"],\n","        generation_config=generation_config,\n","    )[0], \n","    skip_special_tokens=True\n",")\n","\n","print(dash_line)\n","print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"]},{"cell_type":"markdown","metadata":{},"source":["> ☝ **Findings**:\n","- Choosing `max_new_tokens=10` will make the output text too short, so the dialogue summary will be cut.\n","- Putting `do_sample = True` and changing the temperature value you get more flexibility in the output."]},{"cell_type":"markdown","metadata":{},"source":["So, prompt engineering can help improve model performance for this use case, but there are some limitations. In the next lab, I'll explore how to use fine-tuning to help my LLM understand a particular use case in more depth."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":2}
